<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>NHST and screening tests • latent</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="NHST and screening tests">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">latent</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/ci_residuals2.html">Confidence intervals for raw residuals</a></li>
    <li><a class="dropdown-item" href="../articles/NHST_and_screening_tests.html">NHST and screening tests</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-tutorials" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Tutorials</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-tutorials">
<li><a class="dropdown-item" href="../articles/ci_residuals.html">Fitting Latent Models</a></li>
    <li><a class="dropdown-item" href="../articles/ci_residuals.html">Comparing Models</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/Marcosjnez/latent/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>NHST and screening tests</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/Marcosjnez/latent/blob/HEAD/vignettes/NHST_and_screening_tests.Rmd" class="external-link"><code>vignettes/NHST_and_screening_tests.Rmd</code></a></small>
      <div class="d-none name"><code>NHST_and_screening_tests.Rmd</code></div>
    </div>

    
    
<p>When discussing <em>null hypothesis significant testing</em> (NHST),
textbooks often present the type I
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>)
and type II
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>)
errors as the false positive and false negative rates, respectively,
prompting readers to believe they are analogue to the same concepts used
in diagnostic screening. However, in the latter these rates are actually
interpreted to mean the transposed conditionals.</p>
<p>Suppose you are told that 5% of the population develop a disease and
the sensibility of the available diagnostic tools is 90%. In Ioannidis’
world <span class="citation">(Ioannidis, 2005)</span>, the probability
of getting a positive result when the disease is present,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_1)</annotation></semantics></math>,
and when it is absent,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_0)</annotation></semantics></math>,
are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">1-\alpha</annotation></semantics></math>,
respectively, while
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_0 | +)</annotation></semantics></math>
defined in (1) is termed the false positive probability or risk and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_0 | +)</annotation></semantics></math>
is the positive predictive value.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
P(H_0 | +) = \frac{P(+ | H_0) P(H_0)}{P(+ | H_0) P(H_0) + P(+ | H_1) P(H_1)} \qquad (1)
</annotation></semantics></math></p>
<p>This way, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
was 1 - specificity = 0.05, for instance, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">P(H_0 | +) \approx .5</annotation></semantics></math>,
which means that we would be wrong about half of the time when
diagnosing people. However, we encounter several problems if apply this
reasoning to scientific research. The first concern is about the
definition of the prior,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_1)</annotation></semantics></math>.
Ioannidis claims that <em>‘Before running an experiment, investigators
should consider what they believe the chances are that they are testing
a true rather than a non-true relationship’</em> <span class="citation">(Ioannidis, 2005, p. 700)</span>. But this cannot be
accomplished in this framework because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_1)</annotation></semantics></math>
is an statement concerning a parameter in a statistical model not a
substantive research question. Conflating both is a known fallacy.
Concretely,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_1)</annotation></semantics></math>
corresponds to the probability of rejecting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>
when the true parameter is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>
so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_1)</annotation></semantics></math>
cannot be the prior probability of a true finding. But let’s ignore this
caveat. Should then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_1)</annotation></semantics></math>
be the proportion of true hypotheses in a field? How do we define the
bounds of a field and how do we estimate that? Do any others
characteristics like the methodological quality of the research and the
author’s reputation count? Ioannidis does not address any of these
questions to make possible the location of a substantive hypothesis in a
reference group. As a consequence, the ambiguity precludes the method
from stating any rigorous claim because researchers will have a lot of
room for picking ad hoc criteria that may produce a desired result. On
the other hand, it also seems silly that the stringency of the results
of a thorough experiment becomes handicapped by the amount of
independent shoddy research conducted in the field, whatever was the
criterion to define it.</p>
<p>Another problem is interpreting the p-value as a dichotomous result.
For Ioannidis, it does not matter whether the p-value is .49 nor .001,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_0 | +)</annotation></semantics></math>
remains the same. Said another way, effects beyond the critical value
don’t boost the positive predictive value,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_1 | +)</annotation></semantics></math>.
In another paper advocating for the screening testing approach, <span class="citation">Colquhoun (2014)</span> proposed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_1)</annotation></semantics></math>
to be the power for a given effect size (sensibility) and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+| H_0)</annotation></semantics></math>,
the p-value
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mtext mathvariant="normal">specificity</mtext></mrow><annotation encoding="application/x-tex">1 - \text{specificity}</annotation></semantics></math>).
This way,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>+</mi><annotation encoding="application/x-tex">+</annotation></semantics></math>
becomes <em>a more incompatible value with the null than the observed
one</em>. However, this method runs counter a common sense of evidence.
In Figure 1 the false positive risk is plotted as a function of power
for a p-value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>.05</mn><annotation encoding="application/x-tex">.05</annotation></semantics></math>
and prior of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>.5</mn><annotation encoding="application/x-tex">.5</annotation></semantics></math>.</p>
<div class="figure" style="text-align: center">
<img src="NHST_and_screening_tests_files/figure-html/FPR_1-1.png" alt="Figure 1. False positive risk as a function of the test's power for p-value = .05 and prior = .5 (*p-less-than method*)." width="672"><p class="caption">
Figure 1. False positive risk as a function of the test’s power for
p-value = .05 and prior = .5 (<em>p-less-than method</em>).
</p>
</div>
<p>To understand the problem, suppose two experiments yield the same
p-value but the latter has more power for any effect size. Obviously,
the former provides more evidence for an effect because the probability
of the latter to reject the null was higher yet produced the same
p-value. Contrary to this reasoning, we see in the figure that the false
positive risk is inversely proportional to the power, the opposite trend
we expect to find for a sound method of inference. So when a test is
more capable of uncovering an effect, held everything else constant, the
failure to achieve a stronger incompatibility from the null may still
yield a lower false positive risk, wrongly implying that the evidence
for an statistical effect is stronger. Furthermore, notice that either
picking the effect size whose power minimizes the false positive risk or
establishing an alternative a priori makes no sense because we would be
entitled to infer any arbitrary large effect. Therefore, we must use the
observed effect size, in which case observed power replaces power, and
the terms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_0)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_1)</annotation></semantics></math>
are identical up to a transformation, as shown in Figure 2.</p>
<div class="figure" style="text-align: center">
<img src="NHST_and_screening_tests_files/figure-html/FPR_2-1.png" alt="Figure 2. The complementary of the p-value as a function of the observed power for a two-sided z-test." width="672"><p class="caption">
Figure 2. The complementary of the p-value as a function of the observed
power for a two-sided z-test.
</p>
</div>
<p>This situation creates a paradox, first because we actually don’t
have a prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>
until we observe the data. And second, because the smaller the p-value
the larger the observed power, we wrongly conclude that increasing the
specificity also increases the sensibility. The trade-off between both
is reverted. At this point, it’s clear we are not in a diagnostic
context anymore. The terms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_0)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_1)</annotation></semantics></math>
are ill-defined. By the way, recall that by the law of total
probability,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">P(+ | H_0) + P(+ | H_1) = 1</annotation></semantics></math>
if and only if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>
are exclusive and exhaust all possibilities. When the p-value exceeds
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
the likelihood ratio
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><annotation encoding="application/x-tex">\frac{P(+ | H_0)}{P(+ | H_1)}</annotation></semantics></math>
just tells us how many times more we observe
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>+</mi><annotation encoding="application/x-tex">+</annotation></semantics></math>
under
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>
than under
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>
and we then feed this quantity with prior odds to get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_0 | +)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_1 | +)</annotation></semantics></math>.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Odds ratio</mtext></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mtext mathvariant="normal">Odds ratio</mtext><mrow><mn>1</mn><mo>+</mo><mtext mathvariant="normal">Odds ratio</mtext></mrow></mfrac></mtd></mtr></mtable><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\begin{aligned}
\text{Odds ratio} &amp;= \frac{P(+ | H_0)}{P(+ | H_1)} \frac{P(H_0)}{P(H_1)} \\
P(H_0 | +) &amp;= \frac{\text{Odds ratio}}{1 + \text{Odds ratio}}
\end{aligned}
\qquad (2)
</annotation></semantics></math></p>
<p>Anyway, if you pick
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>
after seeing the data then this framework does not fit into your
statistical inquiry. Moreover, this argument shows that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_0 | +)</annotation></semantics></math>
is not the probability you are wrong when claiming an effect. It is
instead the probability you are wrong when claiming an effect in a
universe where there are only two possible statistical hypotheses.</p>
<p>Colquhoun later refused this approach, termed the <em>p-less-than
method</em>, and advocated for the <em>p-equals method</em> <span class="citation">(Colquhoun, 2017)</span>. He argued that we actually
should care about the exact probability density of the results and
interpret
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_0)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(+ | H_1)</annotation></semantics></math>
as the density of the data,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>+</mi><annotation encoding="application/x-tex">+</annotation></semantics></math>,
under the null and under the alternative, respectively. Following the
new approach, Figure 3 displays again the false positive risk against
the power attached to an alternative.</p>
<div class="figure" style="text-align: center">
<img src="NHST_and_screening_tests_files/figure-html/FPR_3-1.png" alt="Figure 3. False positive risk as a function of the test's power for $\alpha = .05$, p-value $= .05$ and prior $= .5$ (*p-equals method*)." width="672"><p class="caption">
Figure 3. False positive risk as a function of the test’s power for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math>,
p-value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">= .05</annotation></semantics></math>
and prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">= .5</annotation></semantics></math>
(<em>p-equals method</em>).
</p>
</div>
<p>The interpretation is exactly as before. We are just counting how
many times more the observed result occurs under the null than under the
alternative and pondering this ratio by prior odds. Nonetheless, in this
case the trend falls in the sort of same mistake as before because for
any fixed p-value the false positive risk should be a monotonic
increasing function of power. Also, whenever we get a significant
result, the effect size that minimizes the false positive risk is the
observed one so we don’t solve any of the previous problems, which are:
the absence of criteria to define the reference group a substantive
hypothesis belongs to, the trend favouring larger effects for a given
p-value (known as the <em>Mountains out of Molehills fallacy</em> <span class="citation">(Mayo, 2018, p. 240)</span>), the ad-hoc choice of the
alternative
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>
after seeing the data that precludes us from calling
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_1)</annotation></semantics></math>
a prior, the inverse proportionality between the observed power and the
p-value that makes the sensibility and specificity positively related
and the assumption that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>
exhaust the whole parameter space.</p>
<p>And there are at least three more reasons to dismiss this approach.
Returning to the screening tests, it is important to understand that we
don’t know the probability that a single person has the disease. Recall
that in this context
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_0)</annotation></semantics></math>
is a frequency and saying that the probability a particular person has
the disease is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
because there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">np</annotation></semantics></math>
people with that disease out of a population of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
people is like saying that a parameter is within a confidence interval
with probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
because the frequency the parameter lies inside is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(H_0 | +)</annotation></semantics></math>
is just an overall probability, an statement about a population. We get
the probability of a false positive not the probability a person got the
disease. So when Ioannidis and Colquhoun say that you can compute the
probability you are wrong given you claim an effect based on a
particular statistically significant result, they actually take what
they believe to be the proportion of wrong hypotheses developed in a
reference group to mean the probability of every single hypothesis from
that group. The second reason is that the p-value and the power of the
test are not conditional probabilities, but this will be explained in a
different post. The third one is that we need to compute no false
probability risk because should never claim a discovery based on a
solely result.</p>
<p>Futhermore, if we do bayesian inference, all this is akin to
suggesting very skeptical priors on the model parameters. This way,
domain expertise necessary for establishing informative priors is
sacrificed in favor of an ill-defined sense that most research
hypotheses, including our, are wrong. My advice is that if you are
interested in quantifying the probability of an hypothesis being true,
go full bayesian instead of resorting to this kind of
<em>frequenstein</em> method.</p>
<p>For another comparison between NHST and screening tests, see <span class="citation">Mayo &amp; Morey (2017)</span>.</p>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-colquhoun_2014" class="csl-entry">
Colquhoun, D. (2014). An investigation of the false discovery rate and
the misinterpretation of p-values. <em>Royal Society Open Science</em>,
<em>1</em>(3), 140216. <a href="https://doi.org/10.1098/rsos.140216" class="external-link">https://doi.org/10.1098/rsos.140216</a>
</div>
<div id="ref-colquhoun_2017" class="csl-entry">
Colquhoun, D. (2017). The reproducibility of research and the
misinterpretation of p-values. <em>Royal Society Open Science</em>,
<em>4</em>(12), 171085. <a href="https://doi.org/10.1098/rsos.171085" class="external-link">https://doi.org/10.1098/rsos.171085</a>
</div>
<div id="ref-ioannidis_2005" class="csl-entry">
Ioannidis, J. P. A. (2005). Why <span>Most Published Research Findings
Are False</span>. <em>PLoS Medicine</em>, <em>2</em>(8). <a href="https://doi.org/10.1371/journal.pmed.0020124" class="external-link">https://doi.org/10.1371/journal.pmed.0020124</a>
</div>
<div id="ref-SIST_2018" class="csl-entry">
Mayo, D. G. (2018). <em>Statistical <span>Inference</span> as
<span>Severe Testing</span>: <span>How</span> to <span>Get Beyond</span>
the <span>Statistics Wars</span></em>. <span>Cambridge University
Press</span>. <a href="https://doi.org/10.1017/9781107286184" class="external-link">https://doi.org/10.1017/9781107286184</a>
</div>
<div id="ref-mayo_2017" class="csl-entry">
Mayo, D. G., &amp; Morey, R. D. (2017). <em>A <span>Poor
Prognosis</span> for the <span>Diagnostic Screening Critique</span> of
<span>Statistical Tests</span></em>. <a href="https://osf.io/nepx9/" class="external-link">https://osf.io/nepx9/</a>
</div>
</div>
</div>
  </main>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Marcos Jiménez.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
