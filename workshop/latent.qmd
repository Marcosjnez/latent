---
title: "latent: an R library for Latent Variable Modeling"
date: "2025-11-10"
author: "Mauricio Garnier-Villarreal, Marcos Jimenez, & Vithor Rosa Franco"

format:
  html:
    number-sections: true
    toc: false
    theme: cosmo       # optional (Bootstrap theme)
    code-fold: true    # optional (folds code blocks)
  pdf:
    pdf-engine: pdflatex
    number-sections: true
    toc: false
    include-in-header:
      text: |
        \usepackage{amsmath,amssymb,amsfonts,mathtools,bm}
        \usepackage{lmodern,graphicx,pdfpages}

fontsize: 12pt
bibliography: references.bib
---

# LCA with latent (categorical & Continuous indicators)

- [Latent Variable Models](#latent-variable-model)
- [Latent Class Analysis (LCA)](#latent-class-analysis-lca)
  - [Person-centered vs Variable-centered](#person-centered-vs-variable-centered)
  - [Terminology](#terminology)
- [`latent`](#latent)
  - [First step: Install and load the package](#latent-installation)
- [Categorical indicator example](#categorical-indicator-example)
  - [`latent` syntax](#latent-syntax)
    - [Model Fit Indices](#model-fit-indices)
    - [Classification Diagnostics](#classification-diagnostics)
    - [Interpreting the Final Class Solution](#interpreting-the-final-class-solution)
  - [Exploratory LCA](#class-enumeration)
- [References](#references)

# Current software limitations for Latent Variable Modeling

* Commercial software (e.g., Mplus) is expensive and not open-source (e.g., LatenGOLD).

* Free and open-source alternatives (e.g., poLCA, depmixS4, lavaan, mirt) are computationally slow and support a limited range of models.

![](\figures\thinking.jpg){width=200px style="vertical-align:middle; margin-right:0.5em;"} So… what are we doing about it?

We are building a free, open-source, efficient, and flexible software. ![](\figures\pirate.jpg){width=200px style="vertical-align:middle; margin-right:0.5em;"}

# The latent R package

::: {.columns}
::: {.column width="70%"}

<details markdown="1">
<summary>Unified and flexible syntax</summary>

- Few arguments for a straightforward analysis  

```{r eval=FALSE, echo=TRUE}
latent::lca(data, nclasses = 1:3)
```

- lavaan syntax for Structural Equation Models

```{r eval=FALSE, echo=TRUE}
HS.model <- ' visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 '
fit <- lcfa(model = HS.model, data = HolzingerSwineford1939)
```

- Customizable models 
![](\figures\fixing.png){width=100%}

</details>

<details markdown="1">
<summary>High-performance computing</summary>

- Core functions written in C++ with the armadillo library
![](\figures\armadillo.png){width=100%}

- Parallelization of multiple random starts to address local maxima  
![](\figures\local_maxima.png){width=100%}

</details>
 
:::
::: {.column width="30%"}
![](\figures\standard.png){width=100%}

![](\figures\load.png){width=100%}
:::
:::

# Latent Variable Models

A latent variable model is a way of connecting things we can measure directly
(called observed or manifest variables) to hidden qualities we cannot measure
directly (called latent variables). These models are used in many areas like
biology, computer science, and social sciences. For example, in psychology they
can take answers from many survey questions and combine them into a smaller set
of traits such as extraversion, and in language analysis they can find hidden
"topics" in large groups of texts. The basic idea is that people's answers on
the observed measures come from their position on the hidden traits, and once
we account for those traits the observed measures are assumed to be unrelated
to each other. Latent variable models can involve either categorical or
continuous observed and hidden variables, as below:

``` {r table01, echo=FALSE}
library(knitr)
library(kableExtra)

latent_table <- data.frame(
  "Latent variables" = c("Continuous", "Categorical"),
  "Continuous (Manifest)" = c("Factor Analysis", "Latent Profile Analysis"),
  "Categorical (Manifest)" = c("Item Response Theory", "Latent Class Analysis"),
  check.names = FALSE
)

knitr::kable(latent_table, "html", caption = NULL) |>
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```

The names used for latent variable models often come from the traditions of the
fields where they were first developed. Therefore, as you might have seen already,
there are several terms related to these types of models. Here we have a short
list of terms to keep in mind when talking about these models

- Mixture: general terms to denote models that identify **unknown
  groups** defined by some probabilistic model.
- LCA: latent class analysis, mixture model that defines a categorical
  latent variable that describes the heterogeneity between groups.
  Usually applied only with categorical indicators
- LPA: latent profile analysis, same as LCA, but usually applied with
  continuous indicators
- Latent variable: an unobserved variable, that cannot be measured with
  direct items. This variable is the reason people answer in a certain
  the way the observed indicators. Corrected for (some) measuremet error
- Indicator: observed item that helps approximate the latent variable

You see that there is a distinction between LCA and LPA, but this is
historical to the time when software could only estimate models with all
indicators being categorical or continuous. Now we can estimate
categorical latent variables with categorical, continuous, or a mix of
these indicators. With our package `latent`, we want to bring all of these
approaches together in one place so that you can run a wide range of latent
variable models without worrying about the jargon from different disciplines.
For this reason, we will use the general term LCA for categorical latent
variable model, independent of the type of indicator. With that in mind,
let’s get started by walking through our first tutorial on LCA.

# Latent Class Analysis (LCA)

Latent class analysis (LCA) is an umbrella term that refers to a number
of techniques for estimating unobserved group membership based on a
parametric model of one or more observed indicators of group membership.
The types of LCA have become quite popular across scientific fields,
most notably finite Gaussian mixture modeling and latent profile
analysis. Vermunt & Magidson (2004) defined LCA more generally as
virtually any statistical model where "some of the parameters [...]
differ across unobserved subgroups".

In general terms, we can think of LCA as an **unknown group** analysis,
where you think that there is heterogeneity in the data due to
differences from these **unknown group**, and we want to first identify
these groups, and describe how they are different between each other.
This way defining the most homogeneous groups, and heterogeneous between
them.

## Person-centered vs Variable-centered

LCA is part of person-centered methods. Person-centered approaches
describe similarities and differences among individuals with respect to
how variables relate with each other and are predicated on the
assumption that the population is heterogeneous with respect with the
relationships between variables. Statistical techniques oriented toward
categorizing individuals by patterns of associations among variables,
such as LCA and cluster analysis, are person-centered. Variable-centered
approaches describe associations among variables and are predicated on
the assumption that the population is homogeneous with respect to the
relationships between variables. In other words, each association
between one variable and another in a variable-centered approach is
assumed to hold for all individuals within the population. Statistical
techniques oriented toward evaluating the relative importance of
predictor variables, such as multivariate regression and structural
equation modeling, are variable-centered (Masyn, 2013).

An interesting extension of this, is that any variable-centered approach
can be made into a person-centered by defining a model that differs in
function of their parameters. For example, we can have a multivariate
regression (variable-centered), and we can have a mixture multivariate
regression (person-centered). Where for the second we assume that the
regression parameters differ across **unknown groups**.

# `latent`

For this tutorial we will use the package `latent`. We developed this package
to have the ability to provide the most adequate estimation methods for all
the latent variable models. In regards to LCA, currently, you can fit a model
with categorical, continuous, or mixed indicators. Other *R* packages, such as
`depmixS4` and `poLCA` allow fitting the same or similar models. However, in
the future, `latent` will also include other types of latent variable models
that are not implemented in these packages. Our goal is to provide users with
a unified platform that contains all the latent variable models that may be of
interest to them, reducing the need to install several different packages.

## First step: Install and load the package

While several functions are already implemented in `latent`, there are still
some necessary improvements before it can be submitted to *CRAN*. Therefore,
you have to follow the installation instructions from the *GitHub* repository: [Marcosjnez/latent](https://github.com/Marcosjnez/latent). If you are using
Linux or Windows, the following code should be enough. If you are using Mac,
please refer to the repository on additional details that may be necessary.

``` {r}
### Install and load latent====
# devtools::install_github("marcosjnez/latent", force = TRUE)
library(latent)
```

# Categorical indicator example

For this analysis, we will employ the **gss82** example data set included in
the `latent` package. Sourced from the `poLCA` package, this data comes from
1,202 white respondents to the 1982 General Social Survey (the data set was
previously featured in McCutcheon, 1987, p. 30).

``` {r}
## Get the dataset from the package
data <- gss82
head(data)
dim(data)
```

Looking into the data attributes, we see that it includes the respondents
opinions on four key areas: the purpose of surveys (good, depends, or a waste),
the accuracy of surveys (mostly true or not true), their understanding of the
questions (good, or fair/poor), and their cooperativeness during the interview
(interested, cooperative, or impatient). 

```{r}
## Print the levels of each variable
lapply(1:ncol(data), function(i) {
  levels(data[,i])
})
```

For didactic purpose, we include missing values randomly to some observations.
Because LCA is a person-centered approach, missing values are also considered
as potential information for clustering individuals.

``` {r}
## Generate 30 random missing observations
# Set seed for reproducibility
set.seed(1234)
# Define the number of missing cases
nmiss <- 30
# Select nmiss rows to transform to NA
missrow <- sample(1:nrow(data), size = nmiss)
# Select nmiss columns to transform to NA
misscol <- sample(1:ncol(data), size = nmiss, replace = TRUE)
# Loop over possible missingness
for(i in 1:nmiss) {
  data[missrow[i], misscol[i]] <- NA
}
```

After including missing values, we can assess the proportion of each response
per variable, achieving less than 1% of missing values for each variable.
Looking at the proportion distribution of the four indicators, we see that
with the exception of the second indicator, all the variables present a major
preference for one answer over another.

```{r}
# Loop over columns to check distribution of data
lapply(1:ncol(data), function(i) {
  table(data[,i], useNA="always") |> prop.table() |> round(digits=3)
})
```

## `latent` syntax

When working with `latent`, we will use the `lca()` function for LCA. If we
look at its helps page `?lca`, the main necessary arguments are: (i) data, the
data frame or matrix with the indicators to be included in the analysis; (ii)
nclasses, the number of latent classes to be estimated; (iii) item, character
vector with the model for each item (i.e., "gaussian" or "multinomial";
defaults to "gaussian" for all the items); (iv) penalties, list of penalty
terms for the parameters; (v) model, list of parameter labels; (vi) control,
list of control parameters for the optimization algorithm; and (vii) do.fit,
TRUE to fit the model and FALSE to return only the model setup (defaults to
TRUE).

In the example below, we create an object called `item` as the character vector
with the model for each item, with all defined as `"multinomial"` (as all items are
discrete). Then, we create a `nclasses` object with an arbitrary value of `3`,
meaning we want the model to estimate three latent classes. We do not change the
`control` argument and use the default value for the `do.fit` argument (as we
want to effectively fit the model to the data). The `penalties` argument indicates
if the prior distribution correction (Vermunt & Magidson, 2016) should be
applied to the log-likelihood.

```{r}
## Define that the items are categorical
item <- rep("multinomial", ncol(data))

## Define the number of latent classes
nclasses <- 3

## Fit the multinomial model to the dataset
fit <- latent::lca(data = data, item = item, nclasses = nclasses,
                   penalties = TRUE, control = NULL, do.fit = TRUE)
```

Congratulation! You have run your first LCA with `latent`. A basic summary of
the results can be returned with the `print` method:

```{r}
# Basic summary
print(fit)
```

The user can also choose to return some specific results/properties of the
model. `latent` uses the `S4` object oriented system and, therefore, you can
check what is returned by its objects using `@`. Below, we show how to extract
the time taken to fit the model, the estimated loglikelihood, the penalized
loglikelihood, and the number of optimization iterations.

```{r}
# Time taken to fit the model
fit@timing
# Estimated loglikelihood
fit@loglik
# Estimated penalized loglikelihood
fit@penalized_loglik
# Number of optimization iterations
fit@Optim$opt$iterations
```

Additionaly, `latent` has some specific functions that help with gathering
useful information resulting from fitting the model to the data, such as fit
indices or fitted latent states/scores. In this case, the structure of the
package resembles the structure of `lavaan`, establishing a common coding
approach for assessing equivalent properties (i.e., latent states/scores,
conditional probabilities/factor loadings) for different types of latent
variable models.

### Fit indices

The `getfit` function is used for extracting several fit indices for the model.
Apart from the model's number of latent classes, number of parameters, and
(penalized) log-likelihood ($LL$), several derived statistics are also provided.
The $L2$ test statistics and the corresponding degrees of freedom ($df$) and *p*-value
are provided for traditional assessment of the model. We also include the $R^2$.
These statistics evaluate the in-sample predictive accuracy, meaning that they
provide an assessment of the models ability to predict the observed outcomes
based on the same data that was use to build up the model.

The other fit indices provided are based on Information Criteria (IC) methods,
which are aimed at evaluating the model's out-of-sample predictive accuracy,
adjusting for overfitting. Ideally, out-of-sample predictive accuracy requires
that we estimate the model with a subsample and them use this fit to
predict the scores of a new sample. However, IC’s approximate this approach
by their different penalty metrics. We discuss IC-based fit indices in more
details in the [Exploratory LCA](#class-enumeration) section. Below, we present
the output of the `getfit` function.

```{r}
## Get fit indices
latent::getfit(fit)
```

### Inspect

`latent` has a `latInspect` function that allows the user to easilly "inspect"
the model for specific informations that could be useful for assessing the model
or even doing follow up analyses. For example, with the argument `what` equal to
`"classes"`, the function will return the marginal probability of each class.
Here we see that the first class concentrates 62% percent of the respondents,
while the other two classes almost equally divide the remainder 38%.

```{r}
## Inspect model objects
# Overall probabilities for each class
latent::latInspect(fit, what = "classes")
```

With the argument `what` equal to `"profile"`, the function will return the
conditional probability of each class per item. Here we see that the Classes
1 and 3 are usually more common for respondents that provide positive attitudes
towards survey taking in all the items. Class 2, on the other hand, is overall
related with negative attitudes towards survey taking in all the items.

```{r}
# The probability of each class per response per item
latent::latInspect(fit, what = "profile")
```

With the argument `what` equal to `"posterior"`, the function will return the
probability of each class given the respondent's response pattern. Because
this is a data set with predictions made for all the respondents, we only
present the first six rows. For all of these respondents, one can see that they
are most likely to be from the first latent group.

```{r}
# Posterior probability for each respondent
head(latent::latInspect(fit, what = "posterior"))
```

With the argument `what` equal to `"state"`, the function will return the
class with the highest probability for each respondent. This vector is usually
applied in follow up inferential analysis to assess how well the classes can be
used to make predictions or to explain another variable of interest. Here, we
simply calculate the observed proportion of each class, which should be similar
to the marginal probability of the classes, as shown before.

```{r}
# Most likely class - the latent "state"
latent::latInspect(fit, what = "state") |> table() |> prop.table() |> round(digits=3)
```

With the argument `what` equal to `"pattern"`, the function will return a data set
with all the observed data patterns and the frequency each of them were observed.
Here we see that three of the six response patterns include `NA`, which are therefore
considered as different patterns which could also be different associated with
one of the latent classes.

```{r}
# All the observed response patterns
head(latent::latInspect(fit, what = "pattern"))
```

With the argument `what` equal to `"table"`, the function will return a complete
data set with additional information about each response pattern. Specifically,
we have the estimated frequency, posterior probability for each class (given the
response pattern), the most likely category, and the log-likelihood specific to
each case, and for each response pattern.

```{r}
# Complete information table for each response pattern
head(latent::latInspect(fit, what = "table"))
```

### Confidence intervals

One can also extract the estimated confidence intervals for any parameter using
the dedicated function `ci`. In the code below, we use the `"standard"` method
for estimating the 95% CIs, and print the results using only 2 `digits`. The
result returned from the function is the same as the `classes` and `profile`
return from the `latInspect` function, but this time including the CIs.

```{r}
## Get confidence intervals
CI <- latent::ci(fit, type = "standard", model = "user",
                 confidence = 0.95, digits = 2)
CI$table
```

## Exploratory LCA

In exploratory LCA, a sequence of models is fitted to the data with each
additional model estimating one more class than the previous model.
These models are then compared and the best solution is selected as the
final class solution. In some cases, prior theory can inform the
researcher about the number of classes to expect.

From a sequence of models, the final class solution is chosen based on
both theoretical and statistical criteria. Theory should drive the
selection of indicator variables, inform the expectations and reflect on
the findings. In addition to this, there are several statistical
criteria to consider in model selection. These include but are not
limited to likelihood ratio tests, information criteria, and the Bayes
factor (Masyn, 2013).

Relative model fit can be examined using the likelihood ratio test. This
is only appropriate when the two models we wish to compare are nested.
The likelihood ratio test statistic is computed as the difference in
maximum log-likelihoods of the two models, with the test degrees of
freedom being the difference in the degrees of freedom of the two
compared models. The test statistic follows the $\chi^2$ distribution,
and we want it to be non-significant in order to give support to the
simpler model. The likelihood ratio test can only compare two nested
models at a time (Collins & Lanza, 2010).

Here we show how to use `latent` syntax in *R* to run the model for a
sequence of LCA’s, with increasing number of classes, from 2 to 6.

```{r}
## Define the number of latent classes
nClassesVec <- 2:6

## Control parameters
control <- list(opt = "lbfgs", rstarts = 100, cores=15)

## Fit the multinomial model to the dataset
fitMult <- latent::lca(data = data, item = item, nclasses = nClassesVec,
                       penalties = TRUE, control = NULL, do.fit = TRUE)
```

The object `fitMult` is a list, where each element is one model fitted with the
given number of latent classes, in the same order as `nClassesVec`. Therefore,
to extract the fit indices for each model, we can process `fitMult` with a `for`
loop or some function from the `apply` family. For our purpose, we use the
`sapply` function to create a matrix with all the fit indices for each model.
With this table, we can use any of the fit indices to decide what is the
optimal number of classes.

```{r}
## Get fit indices per model
fitPerModel <- sapply(fitMult, function(i) {
  latent::getfit(i)
})
# Identify each model per column
colnames(fitPerModel) <- paste0("Model_nC_",nClassesVec)
# Print the resulting table
fitPerModel
```

Fit indices typically used for determining the optimal number of classes
include the Akaike Information Criterion (AIC) and the Bayesian Information
Criterion (BIC). Both information criteria are based on the $Deviance = -2LL$
(which is lower for better fitting models), and add a penalty for the number
of parameters (thus incentivizing simpler models). This helps balance model
fit and model complexity. The lower the value of an information criterion,
the better the overall fit of the model.

In contrast, the in-sample predictive accuracy indices, on the other hand, are
positively biased, meaning that they will present better model fit than it has
in reality. ICs correct for this positive bias by evaluating the models
accuracy approximating the out-of-sample predictive accuracy, meaning that it
is the ability to predict the outcome for observations that are not part of
the training model. For example $R^2$ will increase even if an added predictors
are unnecessary, while IC’s will show worst fit when a predictor
(complexity) is unnecessary (McElreath, 2020)

In comparison to AIC, the BIC applies a stronger penalty for model complexity
that scales logarithmically with the sample size. The literature suggests the
BIC may be the most appropriate information criterion to use for model comparison
(Nylund-Gibson & Choi, 2018; Masyn, 2013). However, that are many alternatives
to the traditional AIC and BIC, including some of the indices already included
in `latent`: AIC3, CAIC, KIC, and SABIC. Each of these are based on different
ways of calculating the complexity penalization.

Because of their different rationales for calculating penalization, information
criteria may occasionally contradict each other, so it is important to identify
a suitable strategy to reconcile them. One option is to select a specific fit
index before analyzing the data. Another option is to always prefer the most
parsimonious model that has best fit according to any of the available fit
indices. Yet another option is to incorporate information from multiple fit
indices using the analytic hierarchy process. Finally, one might make an elbow
plot and compare multiple information criteria (Nylund-Gibson & Choi, 2018).

A disadvantage of the IC’s is that we do not have standard errors for them, so
we only have the absolute values without a measure of their variability. So,
the difference between models IC can be very small, still indicating that the
model with the lower value is “better”, but if this difference is very small
can considered them “functionally equal”, and you should take into consideration
the interpretability of the model.

LCA studies commonly report $-2LL$ of the final class solution. This is a basic
fit measure used to compute most information criteria. However, since log-likelihood
is not penalized for model complexity, it will continuously fall with the
addition of more classes.

Below, we provide a code to extract the ICs for each model fitted to the data.
Then, we create an elbow plot including all the indices. Each index is
represented by a line of a different color. It is possible to see that, for
almost all the models, increasing from 2 to 3 classes provide a better fit.
But increasing to 4 or more classes will usually result in a worse fit. Therefore,
a parsimonious decision in this scenario would be to consider the model with
3 classes to be the best model possible for this data set.

```{r}
## Prepare dataset of fit indices per model
# Indices to compare
compare <- c("AIC", "BIC", "AIC3", "CAIC", "KIC", "SABIC")
color   <- c("red", "gold", "darkgreen", "blue", "magenta", "darkgray")
indices <- fitPerModel[compare,]
forPlot <- data.frame(x=rep(nClassesVec, each=length(compare)),
                      index=compare, y=c(indices))
## Create the plot
# Basic point plot
plot(forPlot$y ~ forPlot$x, xlab="Class Solution", ylab="Value",
     main="Elbow Plot of Information Criteria per Class Solution")
# Add default grid
grid()
# Draw lines for each index
for(i in seq_along(compare)) {
  what <- compare[i]
  lines(forPlot[forPlot$index == what,"y"] ~ forPlot[forPlot$index == what,"x"],
        col=color[i], lwd=2)
}
# Legend for interpretation
legend("topleft", legend=compare, col=color, lty=1, lwd=2, bty="p")
```

# Confirmatory Factor Analysis (CFA)

```{r, eval=FALSE, echo=TRUE}
HS.model <- ' visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 '

fit <- lcfa(model = HS.model, data = HolzingerSwineford1939)

fit@loglik 
fit@penalized_loglik 
fit@loss 
fit@Optim$opt$iterations
fit@Optim$opt$convergence
fit@timing
```

# Multigroup Analysis

```{r, eval=FALSE, echo=TRUE}
model <- 'visual  =~ x1 + x2 + x3
          textual =~ x4 + x5 + x6
          speed   =~ x7 + x8 + x9'

fit <- lcfa(HolzingerSwineford1939, model = model,
            group = "school", estimator = "uls", cor = "pearson",
            std.lv = TRUE, do.fit = TRUE, control = list(opt = "lbfgs"))

fit@loglik 
fit@penalized_loglik 
fit@loss 
fit@Optim$opt$iterations
fit@Optim$opt$convergence
fit@timing
```

## Positive-definite constraints

```{r, eval=FALSE, echo=TRUE}
model <- 'visual  =~ x1 + x2 + x3
          textual =~ x4 + x5 + x6
          speed   =~ x7 + x8 + x9
          x1 ~~ x5
          x1 ~~ x4
          x4 ~~ x5
          x4 ~~ x6'

# With latent:
fit <- lcfa(data = HolzingerSwineford1939, model = model,
            estimator = "ml", cor = "pearson",
            positive = TRUE,
            penalties = list(logdet = list(w = 0.001)),
            std.lv = TRUE, do.fit = TRUE,
            control = list(cores = 20L, rstarts = 20L))

fit@loglik 
fit@penalized_loglik 
fit@loss 
fit@Optim$opt$iterations
fit@Optim$opt$convergence
fit@timing
```

# Work in progress

::: {.columns}
::: {.column width="60%"}

- (Exploratory) Structural Equation Modeling

- (Multidimensional) Item Response Theory

- Hidden Markov Models

Release date? Soon

Download the beta version at https://github.com/Marcosjnez/latent

Contact: m.j.jimenezhenriquez@vu.nl

:::
::: {.column width="40%"}

![](\figures\chef.jpg){width=300px style="vertical-align:middle; margin-right:0.5em;"}

:::
:::

![](\figures\qr_repository.png){width=200px style="vertical-align:middle; margin-right:0.5em;"}
